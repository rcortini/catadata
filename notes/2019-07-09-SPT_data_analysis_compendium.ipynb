{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will make the plots appear directly inside the document\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries we need to perform the analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os, re\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-07-09 SPT data analysis compendium\n",
    "\n",
    "The purpose of this document is to illustrate thoroughly the different steps of the data analysis of Single Particle Tracking (SPT) experiments.\n",
    "\n",
    "**PREMISE**: This tutorial assumes basic knowledge of Python.\n",
    "___\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. **Loading the data**: How to load the data and put it into convenient data structures, taking into account the different requirements of the data analysis.\n",
    "\n",
    "2. **Angle analysis**: we describe how to perform the analysis of the angles of the trajectories.\n",
    "\n",
    "3. **Diffusion coefficient modeling**: we describe how to model diffusion coefficients.\n",
    "\n",
    "----\n",
    "\n",
    "## 1. Loading the data\n",
    "\n",
    "We will look at directories that contain SPT trajectories. The directories will contain sub-directories named such as `Stack016_Cell4`. Inside those directories we will look for different files:\n",
    "\n",
    "1. `Links in tracks statistics.csv` : a CSV-formatted file containing SPT information\n",
    "2. `Spots in tracks statistics.txt` : a TSV-formatted file containing SPT information\n",
    "3. `Spots in tracks statistics_MAP.txt` : a TSV-formatted file containing information on the tracks before the SPT can begin\n",
    "4. `Track statistics.csv` : a CSV-formatted file with information on the quality of the tracks.\n",
    "\n",
    "### 1.1 Loading a single SPT file\n",
    "Let's start by an example. We will write a piece of code that will load the data of a single SPT experiment and put it into a variable that we will call `spt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convenience, we will define a variable that we will call `spt_rootdir`, which is the folder\n",
    "# that contains all the data that we will analyze\n",
    "spt_rootdir = '/home/rcortini/work/CRG/projects/catadata/data'\n",
    "\n",
    "# the file name that we will open\n",
    "mydir = '%s/2_DMSO_R5020_Control/Stack009_Cell3'%(spt_rootdir)\n",
    "spt_fname = '%s/Spots in tracks statistics.txt'%(mydir)\n",
    "\n",
    "print(spt_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we use the `Pandas` Python package to parse the file. We call the `read_csv` function\n",
    "# with the option `sep='\\t'`, which means that the character that separates the fields is a tab\n",
    "# character\n",
    "spt = pd.read_csv(spt_fname, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data of the experiment loaded into the `spt` variable. Let's have a look at how this variable looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable is a Pandas `DataFrame`. The first thing to realize is that we will be interested in the single trajectories, which are identified by having the same `TRACK_ID`. To analyze the trajectories, therefore, we will use the function `groupby` from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt_by_track = spt.groupby('TRACK_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt_by_track.get_group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `spt_by_track` object is difficult to operate with directly. Most of the times we will be interested in the (x, y) coordinates of the trajectories. We can extract this information from this object by performing an iteration over the object. In the following piece of code we will do several things:\n",
    "\n",
    "- initialize an empty list named `trajectories`\n",
    "- unpack the `spt_by_track` structure by iterating over it\n",
    "- extract the `x` and `y` coordinates of the tracks\n",
    "- inserting the track (x, y) coordinates in our list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init empty list\n",
    "trajectories = []\n",
    "\n",
    "# iterate over the tracks. The `track_id` variable is the TRACK_ID field of the data frame.\n",
    "for track_id, track in spt_by_track :\n",
    "    \n",
    "    # extract x and y from the trajectory\n",
    "    x = track['POSITION_X']\n",
    "    y = track['POSITION_Y']\n",
    "    \n",
    "    # here we define the trajectory. We put x and y in a list [x, y], then\n",
    "    # we feed this to the function `np.array` which is numpy's way of initializing\n",
    "    # an array, and then we transpose the array using `.T`. The transpose will make\n",
    "    # our life easier, so we will have an array with N rows and 2 columns, where N is\n",
    "    # the number of steps in the trajectory\n",
    "    this_trajectory = np.array([x, y]).T\n",
    "    \n",
    "    # finally, append the current trajectory to the list of trajectories\n",
    "    trajectories.append(this_trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some basic information on the trajectories. How many trajectories do we have? We can use the function `len` which will give us the length of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the first element of the `trajectories` list, we'll get this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are the (x, y) coordinates of a particle as a function of time. Notice that printing this variable shows `array` at the beginning, which is to say that what we're looking at is a numpy array. Numpy arrays have an important property which is the `shape`, which tells us the dimensions of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular trajectory contains 24 points.\n",
    "\n",
    "We can plot this trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that the `plot` function takes at least two arguments: x and y. We access the x and y\n",
    "# fields of our array by using [:, 0] and [:, 1] respectively. The `:` character means that we\n",
    "# want all the points, whereas the 0 or 1 means we want the column number 0 or 1.\n",
    "plt.plot(trajectories[0][:, 0], trajectories[0][:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a list that contains the number of points in each trajectory in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this way of creating a list is called \"list comprehension\" and is a very useful\n",
    "# syntax feature of Python. It allows to initialize and create a list in one line of code.\n",
    "trajectory_length = [t.shape[0] for t in trajectories]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot a histogram of the trajectory lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trajectory_length, bins=50)\n",
    "plt.xlabel(\"Number of points in track\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Quality control\n",
    "We may want to avoid loading the tracks that do not reach a minimum quality. To do this, we will look at the information stored into the `Track statistics.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's parse the track statistics file. Notice that we do not set the `sep` parameter\n",
    "# as we did before. In fact, this is a CSV file and Pandas will parse it well by default.\n",
    "track_statistics = pd.read_csv('%s/Track statistics.csv'%(mydir))\n",
    "\n",
    "# let's have a look at what it looks like\n",
    "track_statistics[['Label','TRACK_ID','TRACK_MEAN_QUALITY']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the columns of this file is named `TRACK_ID`, and has the same meaning as the one stored in `spt`.\n",
    "\n",
    "Here, we are interested in the parameter `TRACK_MEAN_QUALITY`. We aim at selecting the tracks that have a track mean quality that is over a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, get the a list of the tracks that pass the quality control criterion\n",
    "idx_tracks_to_keep = track_statistics.TRACK_MEAN_QUALITY > 50\n",
    "\n",
    "# then, select the TRACK_ID of the tracks in this list\n",
    "tracks_to_keep = track_statistics[idx_tracks_to_keep].TRACK_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this, we can get the trajectories as we did before, but adding the piece of code that will exclude the tracks that are not in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init empty list\n",
    "trajectories = []\n",
    "\n",
    "# iterate over the tracks. The `track_id` variable is the TRACK_ID field of the data frame.\n",
    "for track_id, track in spt_by_track :\n",
    "    \n",
    "    # skip tracks that did not have sufficiently high average quality\n",
    "    if track_id not in tracks_to_keep :\n",
    "        continue\n",
    "    \n",
    "    # extract x and y from the trajectory\n",
    "    x = track['POSITION_X']\n",
    "    y = track['POSITION_Y']\n",
    "    \n",
    "    # here we define the trajectory. We put x and y in a list [x, y], then\n",
    "    # we feed this to the function `np.array` which is numpy's way of initializing\n",
    "    # an array, and then we transpose the array using `.T`. The transpose will make\n",
    "    # our life easier, so we will have an array with N rows and 2 columns, where N is\n",
    "    # the number of steps in the trajectory\n",
    "    this_trajectory = np.array([x, y]).T\n",
    "    \n",
    "    # finally, append the current trajectory to the list of trajectories\n",
    "    trajectories.append(this_trajectory)\n",
    "    \n",
    "print(\"Loaded %d tracks\"%(len(trajectories)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, many tracks were thrown away because of the quality criterion.\n",
    "\n",
    "## 1.3 A function to load a track\n",
    "Now we can encapsulate the code that we wrote in a single function that will load the data, perform the quality control, and return the list of trajectories that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spt(datadir, quality = None) :\n",
    "    # load the SPT data file\n",
    "    spt_fname = '%s/Spots in tracks statistics.txt'%(datadir)\n",
    "    spt = pd.read_csv(spt_fname, sep='\\t')\n",
    "    \n",
    "    # group by TRACK_ID\n",
    "    spt_by_track = spt.groupby('TRACK_ID')\n",
    "\n",
    "    # get quality of tracks and list of excluded tracks\n",
    "    if quality is not None :\n",
    "        track_statistics = pd.read_csv('%s/Track statistics.csv'%(datadir))\n",
    "        idx_tracks_to_exclude = track_statistics.TRACK_MEAN_QUALITY < quality\n",
    "        tracks_to_exclude = track_statistics[idx_tracks_to_exclude].TRACK_ID\n",
    "    else :\n",
    "        tracks_to_exclude = []\n",
    "    \n",
    "    # extract trajectories\n",
    "    trajectories = []\n",
    "    good_track_ids = []\n",
    "    for track_id, track in spt_by_track :\n",
    "        \n",
    "        # skip tracks that did not have sufficiently high average quality\n",
    "        if track_id in tracks_to_exclude.values :\n",
    "            continue\n",
    "\n",
    "        # extract x and y from the trajectory\n",
    "        x = track['POSITION_X']\n",
    "        y = track['POSITION_Y']\n",
    "\n",
    "        # finally, append the current trajectory to the list of trajectories\n",
    "        trajectories.append(np.array([x, y]).T)\n",
    "        good_track_ids.append(track_id)\n",
    "    \n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this function, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = load_spt(mydir, quality = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish this part, we can plot *all* the trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trajectory in trajectories :\n",
    "    plt.plot(trajectory[:,0], trajectory[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Bulk loading of trajectories\n",
    "Armed with the function that loads the SPT data of one experiment, we can now proceed to loading *all* the data from a series of experiments. To do that, we will define a \"master\" directory in which the program will look for sub-directories that match a certain pattern in their names. For each directory, we will invoke the function that loads the data. We must keep the data tidy, so we will define appropriate data structures to keep the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the master directory\n",
    "data_rootdir = '%s/2_DMSO_R5020_Control'%(spt_rootdir)\n",
    "\n",
    "# we will keep our data in a dictionary\n",
    "experiments = {}\n",
    "\n",
    "# iterate over the subdirectories. Notice that the variable `subdir` will contain only\n",
    "# the name of the subdirectory, not the full path of it.\n",
    "for subdir in os.listdir(data_rootdir) :\n",
    "    \n",
    "    # here we test the name of the directory. We use a regular expression to check\n",
    "    # whether the name of the subdirectory contains a format \"StackN_CellN\".\n",
    "    if re.match('Stack[0-9]+_Cell[0-9]+', subdir) is None :\n",
    "        continue\n",
    "    \n",
    "    # if we get here, we passed the test\n",
    "    experiments[subdir] = load_spt('%s/%s'%(data_rootdir, subdir), quality=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get information on the first few experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for name, trajectories in experiments.items() :\n",
    "    print(name, len(trajectories))\n",
    "    n += 1\n",
    "    if n == 5 : break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get overall statistics on the distribution of the number of tracks and their lengths. First, we create a list that contains the length of the tracks and the number of trajectories for each cell, and then another list that contains the number of points in each trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_lengths = []\n",
    "n_tracks = []\n",
    "for name, trajectories in experiments.items() :\n",
    "    \n",
    "    # here we use \"extend\" so that the list will contain a list of number and\n",
    "    # not a list of lists\n",
    "    track_lengths.extend([len(t) for t in trajectories])\n",
    "    \n",
    "    n_tracks.append(len(trajectories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of number of points in trajectories\n",
    "plt.hist(track_lengths, bins = 80)\n",
    "plt.xlabel(\"Number of points in track\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of the number of trajectories per cell\n",
    "plt.hist(n_tracks, bins=40)\n",
    "plt.xlabel(\"Number of tracks in cell\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can create a single function that takes care of getting all the data in bulk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiments(data_rootdir, quality=None) :\n",
    "    \n",
    "    # init the output data structure\n",
    "    experiments = {}\n",
    "\n",
    "    # iterate over the subdirectories. Notice that the variable `subdir` will contain only\n",
    "    # the name of the subdirectory, not the full path of it.\n",
    "    for subdir in os.listdir(data_rootdir) :\n",
    "\n",
    "        # here we test the name of the directory. We use a regular expression to check\n",
    "        # whether the name of the subdirectory contains a format \"StackN_CellN\".\n",
    "        if re.match('Stack[0-9]+_Cell[0-9]+', subdir) is None :\n",
    "            continue\n",
    "\n",
    "        # if we get here, we passed the test\n",
    "        experiments[subdir] = load_spt('%s/%s'%(data_rootdir, subdir), quality=50)\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the function to load the data in bulk. Let's load the data, and move on to the data analysis now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = load_experiments(data_rootdir, quality=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Angle Analysis\n",
    "\n",
    "The next step will be to look at the distribution of angles in the trajectories. The definition of the angles will be performed as in the following graph.\n",
    "\n",
    "![Alt text](angles.svg)\n",
    "\n",
    "In the figure, the black circles are localizations, each of which have cartesian coordinates $(x, y)$. As a function of time, the particle's position is described by the vectors $\\vec{r}(t_1) \\equiv \\vec{r}_1$, $\\vec{r}(t_2) \\equiv \\vec{r}_2$, ...\n",
    "\n",
    "To calculate the angles, we need *three* localizations: the first two to define the first *displacement vector* $\\vec{r}_{12} = \\vec{r}_2 - \\vec{r}_1$, and the second and third localizations are used to define the second displacement vector $\\vec{r}_{23} = \\vec{r}_3 - \\vec{r}_2$. Once we have the two vectors $\\vec{r}_{12}$ and $\\vec{r}_{23}$, we can calculate the angle between the two vectors.\n",
    "\n",
    "To do this properly, we need to remember the formula for the scalar product of two vectors:\n",
    "$\\vec{v}_1 \\cdot \\vec{v}_2 = |\\vec{v}_1| |\\vec{v}_2| \\cos(\\theta_{12})$. Therefore, the *absolute value* of the angle between the two vectors can be calculated as\n",
    "\n",
    "$|\\theta_{12}| = \\arccos \\frac{\\vec{v}_1 \\cdot \\vec{v}_2}{|\\vec{v}_1| |\\vec{v}_2|}$.\n",
    "\n",
    "The last step is to give a sign to the vector. The trick is to remember that the *vector product* of two vectors in two dimensions points to one direction or the other depending on the relative orientation of the two vectors. For example, if the first vector lays on the $x$-axis ($\\vec{v}_1 \\equiv (1,0)$), and the second vector bisects the first quadrant ($\\vec{v}_2 \\equiv (1,1)$), the vector product would be point upwards. In case that the second vector bisects the fourth quadrant ($\\vec{v}_2 = (1, -1)$) then the vector product would be point downwards. Since the two vectors are in two dimensions, the vector product only has a $z$ component. So the final formula for the signed angle between the vectors is:\n",
    "\n",
    "$\\theta_{12} = \\mathrm{sign}(\\vec{v}_1 \\times \\vec{v}_2) \\arccos \\frac{\\vec{v}_1 \\cdot \\vec{v}_2}{|\\vec{v}_1| |\\vec{v}_2|}$.\n",
    "\n",
    "With these considerations, let's write down the function that calculates the angles. First, let's write a function that calculates the angle between two vectors `v1` and `v2` (in 2D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle(v1, v2) :\n",
    "    # calculate the dot product between the two vectors\n",
    "    d = np.dot(v1, v2)\n",
    "    if d==1.0 :\n",
    "        return 0.0\n",
    "    \n",
    "    # calculate the z component of the vector product of the two vectors\n",
    "    v = v1[0]*v2[1] - v1[1]*v2[0]\n",
    "        \n",
    "    # calculate the norm of the two vectors\n",
    "    nv1 = np.linalg.norm(v1)\n",
    "    nv2 = np.linalg.norm(v2)\n",
    "    \n",
    "    # we add a check of sanity: if any of the two difference vectors has\n",
    "    # zero length, then it means that the particle has not moved, and the angle\n",
    "    # cannot be defined.\n",
    "    if nv1 == 0.0 or nv2 == 0.0 :\n",
    "        return None\n",
    "    \n",
    "    # calculate the cosine of the angle\n",
    "    N = d/(nv1*nv2)\n",
    "    \n",
    "    # and finally the signed angle\n",
    "    return np.sign(v)*np.arccos(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to calculate the angles for a single trajectory. To do that, we need to calculate the _difference between the position vectors_ as explained above. To do that, we use Numpy's `diff` function, which takes the difference between values in an array along a specified dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angles_in_trajectory(trajectory) :\n",
    "    angles = []\n",
    "    \n",
    "    # calculate the x and y components of the difference vectors. Note that we\n",
    "    # only take from the second element of the list onwards, because the first one\n",
    "    # is always (0, 0)\n",
    "    delta_X = np.diff(trajectory[:,0], axis=0)[1:]\n",
    "    delta_Y = np.diff(trajectory[:,1], axis=0)[1:]\n",
    "    \n",
    "    # let's now create an array of all the difference vectors. We do the transpose\n",
    "    # to create a vector that has many rows and 2 columns\n",
    "    T = np.array([delta_X, delta_Y]).T\n",
    "    \n",
    "    # now we do a loop to calculate the angles between the vectors. Note that we start\n",
    "    # the iteration from the second element of the list.\n",
    "    for i in range(1, len(T)) :\n",
    "        a = angle(T[i], T[i-1])\n",
    "        if a is not None :\n",
    "            angles.append(a)\n",
    "    return angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this on a single trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the angles in a single trajectory\n",
    "trajectory = experiments['Stack001_Cell2'][0]\n",
    "a = angles_in_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a nice plot of the trajectory and print out the values of the angles\n",
    "plt.plot(trajectory[:,0], trajectory[:,1], 'o--', markersize=8)\n",
    "plt.xlabel(\"X [microns]\", fontsize=24)\n",
    "plt.ylabel(\"Y [microns]\", fontsize=24)\n",
    "for i in range(trajectory.shape[0]) :\n",
    "    plt.text(trajectory[i,0], trajectory[i,1], i+1, fontsize=14)\n",
    "    if i>1 and i<trajectory.shape[0]-1 :\n",
    "        print(\"theta_%d = %.4f radians (%.2f degrees)\"%(i-1,a[i-2],a[i-2]*180/np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here the sign of the angles, and that for a trajectory with 13 points we have 10 angles.\n",
    "\n",
    "The next step is to do the calculation of the angles for all the trajectories, for all the cells, for all the experiments. Since we need to do a histogram of the values of the angles, we need to accumulate all the values of the angles. For that, we will do an `extend` of the python lists in an iteration over all the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = []\n",
    "for experiment_name, trajectories in experiments.items() :\n",
    "    for n, trajectory in enumerate(trajectories) :\n",
    "        a = angles_in_trajectory(trajectory)\n",
    "        if a is None :\n",
    "            print(experiment_name, n)\n",
    "            break\n",
    "        angles.extend(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a plot with the distribution of the angles. We will do a nice radial plot, like the pros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_angles(angles, bins, title) :\n",
    "    # the first thing is to create the histogram of the angles, with the specified\n",
    "    # number of bins\n",
    "    counts, angle_edges = np.histogram(angles, bins = bins)\n",
    "    \n",
    "    # create a vector corresponding to the centers of the bins\n",
    "    angle_centers = angle_edges[1:] - np.ediff1d(angle_edges)\n",
    "    \n",
    "    # init the figure\n",
    "    fig = plt.figure(figsize = (5,5))\n",
    "    \n",
    "    # here we initialize a plot with polar projection\n",
    "    ax = plt.subplot(111, projection = 'polar')\n",
    "    \n",
    "    # plot the histogram\n",
    "    bars = ax.bar(angle_centers, counts, width = 2*np.pi/bins, edgecolor = 'k')\n",
    "    \n",
    "    # finishing touches\n",
    "    ax.set_title(title, y = 1.1, fontsize = 18)\n",
    "    ax.set_yticklabels([])\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_angles(angles, 20, 'Example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing the same analysis for the different experiments, we'll obtain the figures with the angle analysis for all the experiments.\n",
    "\n",
    "## 3 Diffusion analysis\n",
    "\n",
    "The diffusion analysis consists in several parts:\n",
    "\n",
    "1. Analysis of the **displacements**: we first make an histogram of the square displacements of the particles, and then fit a model that takes into account different populations of diffusing particles to extract the diffusion coefficients and size of the populations.\n",
    "2. Analysis of the **mean square displacement (MSD)** as a function of time, which will allow us to extract the anomalous diffusion parameter, and the mean diffusion coefficient.\n",
    "\n",
    "### 3.1 Analysis of the displacements\n",
    "\n",
    "Let's take a population of particles that that diffuses with brownian motion in two dimensions. At any time, a particle starting at the origin of a reference frame will be encountered at time $\\Delta t$ in a shell of radius $r$ and thickness $\\mathrm{d} r$ with probability given by:\n",
    "\n",
    "$p(r^2, \\Delta t) \\mathrm{d} r^2 = \\frac{1}{4 D \\Delta t} \\exp (-\\frac{r^2}{4 D \\Delta t})$\n",
    "\n",
    "where $D$ is the diffusion coefficient. One option would therefore be to calculate the distribution of $r^2$ values and fit this formula to it. However, this would depend on the binning chosen to calculate the histogram of the values of $r^2$. A better option is to fit the *cumulative distribution* of the square displacements, which does *not* depend on  the binning. The cumulative distribution is the integral of the above expression, and is given by\n",
    "\n",
    "$P(r^2, \\Delta t)= 1 - \\exp (-\\frac{r^2}{4 D \\Delta t})$\n",
    "\n",
    "With this expression we can fit a model of diffusion and extract the diffusion coefficient.\n",
    "\n",
    "Consider then the case that there are $N$ different populations of diffusing particles, where the $i$-th particle type takes up a fraction $f_i$ of the population, and has diffusion coefficient $D_i$. The cumulative distribution of square displacements for the mixed population will be\n",
    "\n",
    "$P(r^2, \\Delta t)= 1 - \\sum_{i=1}^m f_i \\exp (-\\frac{r^2}{4 D_i \\Delta t})$\n",
    "\n",
    "This last formula is the one that we will use for our fits.\n",
    "\n",
    "The first step now is to extract the distribution of $r^2$ values. Let's illustrate how to do this for a single trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the angles in a single trajectory\n",
    "trajectory = experiments['Stack001_Cell2'][0]\n",
    "\n",
    "# step 1: get the displacements. In a first step we perform the difference between\n",
    "# the positions of the particles using diff, and in the second step we calculate the\n",
    "# norm of the displacement vectors\n",
    "r = np.linalg.norm(np.diff(trajectory, axis = 0), axis = 1)\n",
    "\n",
    "# step 2: get the square displacements, simple enough\n",
    "r2 = r**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take these pieces of code and put them into a function that will get us the square displacements for a bunch of trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = []\n",
    "for experiment_name, trajectories in experiments.items() :\n",
    "    for n, trajectory in enumerate(trajectories) :\n",
    "        r = np.linalg.norm(np.diff(trajectory, axis = 0), axis = 1)\n",
    "        r2.extend(r**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list that contains all the square displacement values for all the experiments in our list. Let's plot the histogram of square displacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(r2, bins=50)\n",
    "plt.xlabel(r\"r2 [microns 2]\", fontsize=24)\n",
    "plt.ylabel(r\"Counts\", fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compute the cumulative distribution. To do that, we will calculate the *cumulative sum* of the histogram and divide by the total number of events. We need to pass these data points then to a fitting function that we will write later. This means that we need to have a vector of values of $x$ and $y$: the $x$ values will be the values of the square displacements, the $y$ values will be the values of the cumulative sum. We need to take care of a few subtleties in this. First, when we use the function `cumsum` of Numpy, this calculates a vector that has the same number of dimensions as the original vector, as in the toy example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([9., 2., 3.])\n",
    "np.cumsum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is actually a vector that also contains the point $(0, 0)$.\n",
    "\n",
    "Second, when we invoke the `histogram` function of Numpy, it returns two vectors: `h` and `bins`. The former is the value of the counts associated to each bin, whereas the latter corresponds to the *edges* of the bins. Therefore, the number of points of `bins` will be greater by one compared to `h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the minimum and maximum value of the square displacement that we want\n",
    "# to study\n",
    "m = 0\n",
    "M = 1.1\n",
    "\n",
    "# number of points to include in the final histogram\n",
    "N = 100\n",
    "\n",
    "# calculate the histogram of square displacements\n",
    "bins = np.linspace(m, M, N)\n",
    "h, bins = np.histogram(r2, bins=bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to prepare the $x$ and $y$ vectors for fitting. We will define the right edges of the bins as $x$, and the cumulative sum of the histogram as $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare x and y for fitting\n",
    "x = np.zeros(N)\n",
    "y = np.zeros(N)\n",
    "x[1:] = bins[1:]\n",
    "y[1:] = np.cumsum(h)/sum(h)\n",
    "\n",
    "# and plot\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"Square displacement [microns 2]\", fontsize=24)\n",
    "plt.ylabel(\"Cumulative distribution\", fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to do the fitting.\n",
    "\n",
    "For fitting, we need three ingredients:\n",
    "1. the data to fit\n",
    "2. the model that we want to fit\n",
    "3. a function that takes the data and the model, and returns the optimal parameters\n",
    "\n",
    "We have the data already. Let's work on the model. To fit the model, we will use the `curve_fit` function from `scipy`. This function accepts as an argument the function to fit, which must be of the form `f(X, *p)`, where `X` contains the data to fit, and `p` the parameters to fit.\n",
    "\n",
    "Let's write the model that we want to fit so that `curve_fit` will accept it. One must keep in mind that the `X` part of the arguments contains not only the `x` values that the function must calculate, but also any potential constants that the model needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_model_1s(X, *p) :\n",
    "    \"\"\"\n",
    "    One species diffusion model\n",
    "    \"\"\"\n",
    "    # let's extract data and contants\n",
    "    x = X[0]         # data to fit\n",
    "    dt = X[1]        # Delta t values\n",
    "    \n",
    "    # extract the parameters\n",
    "    D = p[0]\n",
    "    \n",
    "    # calculate the result of the model\n",
    "    y = 1-np.exp(-x/(4*D*dt))\n",
    "    return y\n",
    "    \n",
    "def diffusion_model_2s(X, *p) :\n",
    "    \"\"\"\n",
    "    Two species diffusion model\n",
    "    \"\"\"\n",
    "    # let's extract data and contants\n",
    "    x = X[0]         # data to fit\n",
    "    dt = X[1]        # Delta t values\n",
    "    \n",
    "    # extract the parameters\n",
    "    D1 = p[0]\n",
    "    D2 = p[1]\n",
    "    f1 = p[2]\n",
    "    \n",
    "    # calculate the result of the model\n",
    "    y = 1-f1*np.exp(-x/(4*D1*dt))-(1-f1)*np.exp(-x/(4*D2*dt))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data to fit\n",
    "dt = 0.015         # in seconds... = 15ms\n",
    "X = (x, dt)\n",
    "\n",
    "# one species diffusion model fit\n",
    "p0_1s = 5.0      # initial guess for the diffusion coefficient\n",
    "coeff_1s, var_matrix_1s = curve_fit(diffusion_model_1s, X, y, p0=p0_1s)\n",
    "\n",
    "# two species diffusion model fit: note that here we use the \"bounds\" option for\n",
    "# curve_fit: there is otherwise the chance that the values of f1 will be outside of\n",
    "# its legitimate bounds [0, 1]\n",
    "p0_2s = [5.0, 1.0, 0.1]     # initial guess for D1, D2, and f1, respectively\n",
    "coeff_2s, var_matrix_2s = curve_fit(diffusion_model_2s, X, y, p0=p0_2s,\n",
    "                                   bounds = (0,\n",
    "                                             [np.inf, np.inf, 1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore the results of the fit. Let's plot the data together with the best fit for the one-species and the two-species model. We'll also print out the values of the parameters on the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init figure\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "\n",
    "# plot values of the data and the fitted values\n",
    "plt.plot(x, y, 'o', markersize=5, label='Data')\n",
    "plt.plot(x, diffusion_model_1s((x, dt), coeff_1s[0]), label='One species fit')\n",
    "plt.plot(x, diffusion_model_2s((x, dt), coeff_2s[0],\n",
    "                                  coeff_2s[1],\n",
    "                                  coeff_2s[2]), label='Two species fit')\n",
    "\n",
    "# aesthetic touches\n",
    "plt.title(\"Example\", fontsize=24)\n",
    "plt.axhline(y = 1.0, linestyle = '--', color = 'k', linewidth = 0.75)\n",
    "plt.xlabel(\"Displacement squared [$\\mu$m$^2$]\")\n",
    "plt.ylabel(\"Cumulative distribution\")\n",
    "plt.xlim(m, M)\n",
    "plt.xticks(np.arange(0., M, 0.25))\n",
    "plt.legend(loc='lower right', fontsize = 16)\n",
    "\n",
    "plt.text(0.25, 0.8, \"One species fit: D = %.3f $\\mu m^2/s$\"%(coeff_1s[0]), fontsize = 14)\n",
    "plt.text(0.25, 0.45, \"Two species fit: D1 = %.3f $\\mu m^2/s$\\n\"\n",
    "                      \"                         D2 = %.3f $\\mu m^2/s$\\n\"\n",
    "                      \"                         f1 = %.3f\"%(coeff_2s[0],\n",
    "                                                            coeff_2s[1],\n",
    "                                                            coeff_2s[2]), fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the part on the diffusion analysis.\n",
    "\n",
    "### 3.2 Mean square displacement analysis\n",
    "\n",
    "The next part is to study the mean square displacement (MSD) of the trajectories. The objective is to extract the diffusion coefficient and the anomalous exponent of the diffusion. For a particle that diffuses with anomalous diffusion in 2D the MSD will depend on the temporal interval as follows:\n",
    "\n",
    "$MSD(t) = 4 D_\\alpha t^\\alpha$,\n",
    "\n",
    "where $\\alpha$ is the anomalous exponent, that takes the value of $\\alpha = 1$ for normal Brownian diffusion. Strictly speaking, the dimensions of the diffusion coefficient are of $[L]^2/[T]$ only if $\\alpha = 1$. That's why we indicate the diffusion coefficient as $D_\\alpha$.\n",
    "\n",
    "To obtain the value of $\\alpha$, we first need to calculate the MSD, and then fit it to the formula above.\n",
    "\n",
    "The tricky part is to calculate the MSD. The single particle tracking experiments are performed by taking images every $\\Delta t = 15ms$. To get the MSD, we need averages at fixed time intervals. Say that a trajectory contains 10 points. Clearly, there will be 9 points that are at $\\Delta t$ time difference, there are 8 points that are at $2 \\Delta t$ time difference, 7 at $3\\Delta t$, and so on. Only one point in the trajectory is at $9 \\Delta t$. This is why the calculation of the MSD will yield accurate estimations at small time intervals, and progressively less and less accurate estimations at larger time intervals.\n",
    "\n",
    "This also means that for every trajectory that has $T$ time points, we will be able to calculate $T-1$ points for $\\Delta t$, $T-2$ for $2 \\Delta t$, and so on. At the end, we will perform the averages. We will do this by creating a square matrix of $T-1 \\times T-1$ dimensions. The matrix elements will be defined as follows:\n",
    "\n",
    "$A_{ij} = |\\vec{r}(t_i) - \\vec{r}(t_i + j \\Delta t)|^2$,\n",
    "\n",
    "that is, the columns contain all the $r^2$ values for a fixed time interval, and the row contain all possible values for that time interval.\n",
    "\n",
    "To avoid doing cumbersome calculations, we will fill the matrix at the beginning with `NaN`s, and then use Numpy's `nanmean` function to perform the averages: this way, it will automatically perform the average taking into account that only some of the values have been calculated.\n",
    "\n",
    "With that said, let's look at how we can calculate the values of the MSD for a single trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msd_trajectory(trajectory) :\n",
    "    # get the length of the trajectory\n",
    "    T = trajectory.shape[0]\n",
    "\n",
    "    # initialize a matrix that will allow us to calculate the MSD: the matrix\n",
    "    # has T-1 rows, corresponding to each time point except the last one (because\n",
    "    # for the last one we cannot calculate any difference in time); it has then\n",
    "    # T-1 columns, so that it stores all the possible time intervals from 1 to T\n",
    "    delta = np.zeros((T-1, T-1))\n",
    "    delta.fill(np.nan)\n",
    "\n",
    "    # calculate the MSD for the single trajectory: iterate first on the initial time\n",
    "    # point, and then on the values of the possible time intervals\n",
    "    for t0 in range(T-1) :\n",
    "        for delay in range(1,T-t0) :\n",
    "            # get value of second time point\n",
    "            t1 = t0 + delay\n",
    "            \n",
    "            # get positions of the particle at the two time points\n",
    "            pos1 = trajectory[t1,:]\n",
    "            pos0 = trajectory[t0,:]\n",
    "            \n",
    "            # calculate square displacement\n",
    "            delta[t0, delay-1] = np.sum((pos1-pos0)**2)\n",
    "\n",
    "    # now we can return the mean of this track\n",
    "    return np.nanmean(delta, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test now this function with a single trajectory as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trajectory[:,0], trajectory[:,1])\n",
    "msd_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can perform the averages over all the trajectories in an experiment set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function that we will use to calculate the MSD\n",
    "def msd_t(trajectories) :\n",
    "    \n",
    "    # init the variables\n",
    "    ntrajectories = len(trajectories)\n",
    "    lengths = [t.shape[0] for t in trajectories]\n",
    "    max_t = int(max(lengths)) - 1\n",
    "    \n",
    "    # We init a matrix that will contain the MSD calculated for each of the\n",
    "    # tracks. It will have `ntracks` rows and `max_t - 1` columns.\n",
    "    MSD = np.zeros((ntrajectories, max_t))\n",
    "    MSD.fill(np.nan)\n",
    "\n",
    "    # iterate through the all the tracks\n",
    "    for i, trajectory in enumerate(trajectories) :\n",
    "        m = msd_trajectory(trajectory)\n",
    "        MSD[i, 0:len(m)] = m\n",
    "    \n",
    "    # return the MSD\n",
    "    return np.nanmean(MSD, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = []\n",
    "for experiment, tracks in experiments.items() :\n",
    "    trajectories.extend(tracks)\n",
    "msd = msd_t(trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the MSD data. Let's have a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(msd)\n",
    "plt.xlabel(r'$\\log[\\Delta t]$ [s]', fontsize = 18)\n",
    "plt.ylabel(r'$\\log [MSD (\\Delta t)]$', fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite clear that the data is smooth in the first part, and noisy in the second part. Let's move on to fitting this data. The best way to fit this is to take into consideration that we can transform the MSD law into the following form:\n",
    "\n",
    "$\\log MSD = \\log (4D_\\alpha) + \\alpha \\log t$\n",
    "\n",
    "which means that we can fit a linear law of $\\log MSD$ as a function of $\\log t$, in which $\\log 4D_\\alpha$ is the intercept, and $\\alpha$ is the slope of the line. Note that this will be possible only in a certain range of values of $t$.\n",
    "\n",
    "To fit the data, we write a custom linear regression function, that is handy because it returns the errors on the estimates of the parameters, and can be supplied with the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression (x,y,prob) :\n",
    "    \"\"\"\n",
    "    Fit (x,y) to a linear function, using maximum likelihood estimation of the\n",
    "    confidence intervals on the coefficients, given the user-supplied\n",
    "    probability *prob*\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    xy = x*y\n",
    "    xx = x*x\n",
    "    # estimates\n",
    "    xmean = x.mean()\n",
    "    ymean = y.mean()\n",
    "    xxmean = xx.mean()\n",
    "    xymean = xy.mean()\n",
    "    b1 = (xymean-xmean*ymean) / (xxmean-xmean**2)\n",
    "    b0 = ymean-b1*xmean\n",
    "    s2 = 1./n * sum([(y[i] - b0 - b1 * x[i])**2 for i in range(n)])\n",
    "    #confidence intervals\n",
    "    alpha = 1 - prob\n",
    "    c1 = stats.chi2.ppf(alpha/2.,n-2)\n",
    "    c2 = stats.chi2.ppf(1-alpha/2.,n-2)\n",
    "    # get results and return\n",
    "    c = -1 * stats.t.ppf(alpha/2.,n-2)\n",
    "    bb1 = c * (s2 / ((n-2) * (xxmean - (xmean)**2)))**.5\n",
    "    bb0 = c * ((s2 / (n-2)) * (1 + (xmean)**2 / (xxmean - xmean**2)))**.5\n",
    "    return b0,b1,bb0,bb1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first step: prepare the data to be fitted\n",
    "# remember that dt was defined earlier\n",
    "cutoff = 8      # number of points to fit\n",
    "t = np.arange(1, msd.size+1)*dt\n",
    "x = np.log(t[:cutoff])\n",
    "y = np.log(msd[:cutoff])\n",
    "\n",
    "# now we can fit the model y = a + b * x, where da and db will be the standard\n",
    "# errors on the estimates of the coefficients of the regression\n",
    "b, alpha, db, dalpha = linear_regression(x, y, 0.99)\n",
    "\n",
    "# and finally we transform back to get the values of diffusion coefficient\n",
    "# with its error\n",
    "D = np.exp(b)/4.\n",
    "dD = np.exp(db)/4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# t axis\n",
    "t = dt*np.arange(cutoff)\n",
    "\n",
    "# the diffusion model: 2D diffusion\n",
    "yfit = 4 * D * t ** alpha\n",
    "\n",
    "# plot the data and the fit to the model\n",
    "plt.loglog(dt*np.arange(msd.shape[0]), msd,\n",
    "      markersize = 8)\n",
    "plt.loglog(t, yfit, '--', color = 'k')\n",
    "\n",
    "plt.text(0.02, 0.01, r'$\\alpha$ = %.3f +/-'\n",
    "       '%.3f\\nD = %.3f +/- %.3f $\\mu m^2$/s'%(alpha, dalpha, D, dD), fontsize=18)\n",
    "\n",
    "# finish up with plot style\n",
    "plt.xlabel(r'$log[\\Delta t]$ [s]', fontsize = 18)\n",
    "plt.ylabel(r'$\\log [MSD (\\Delta t)]$', fontsize = 18)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "vpython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
